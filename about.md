---
layout: page
title: About me
permalink: /about/
---

## Research interests

Roughly in order of familiarity:

- Optimization
  - Search algorithms ([mine Game Tree Search Algorithms library](https://github.com/AdamStelmaszczyk/gtsa))
  - Heuristic & metaheuristic algorithms
- Machine learning
  - Reinforcement learning
  - Neural networks
- Computer games
- Game theory
- AI safety

## Research ideas

1. "On the shoulders of giants". Get inspired from works of e.g.:

   David Silver, Timothy Lillicrap, Doina Precup, Richard Sutton, 
   Jürgen Schmidhuber, Sepp Hochreiter, 
   John Schulman, Chelsea Flynn, Sergey Levine, Volodymyr Mnih, 
   Jonathan Schaeffer, David Churchill

2. "Different ways of training neural nets". Improving the gradient methods, or non-gradient ones, e.g. neuroevolution.

   Neuroevolution papers:

   2017 OpenAI _Evolution Strategies as a Scalable Alternative to Reinforcement Learning_  
   2017 Uber _Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning_  
   2021 Nature _Correspondence between neuroevolution and gradient descent_ ❤️

   TODO: read Jagodziński's 2021 _Deep Neuroevolution: Training Neural Networks Using a Matrix-Free Evolution Strategy_, but need a PDF.

3. "There is model-based reinforcement learning, so let's do model-based evolution strategies".
Building an approximate model during the evolutionary search, e.g. to have a faster/cheaper simulator or to improve the search.

4. "Higher abstraction layer". From the lowest layer we have: hardware,
programming languages, machine learning... maybe we could add something useful on top?
Go more meta? Maybe not fitting one function with some data, but
multiple functions / architectures / algorithms... Or we could focus more on data part, e.g. AI finding out more data by itself or using it more efficiently.

5. Study metaheuristics from [wiki](https://en.wikipedia.org/wiki/Table_of_metaheuristics).

6. "See how current AI is different from humans and imitate that". Examples:
  
   - Babies learn a lot unsupervised by playing and exploring on their own.
   - Human brain can repeair.
   - Only choosen connections in the brain are strenghten.
   - Newborn brain is not initiated with something random.
  
   A paper listing those differences would be also great.

## Publications

2017, [_Learning to Run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments_](https://arxiv.org/abs/1804.00361), [https://github.com/AdamStelmaszczyk/learning2run](https://github.com/AdamStelmaszczyk/learning2run)

2014, Master's thesis, [_DE/mid – new variant of differential evolution algorithm using the midpoint of the population_](https://github.com/AdamStelmaszczyk/masters-thesis)
